#The purpose of this script is to find all the unique combinations of median
#denudation and lithology, so we can generate CSVs of quantiles against
#denudation rate values (quantile function). These CSVs can then be read into a 
#python script where called upon, and used to interpolate an erosion rate value 
#based on a randomly generated quantile.

setwd("C:/Users/zonde")

##Required Libraries (use install.packages to install these)
library(sp)
library(raster)
library(rgdal)
library(rgeos)
library(GISTools)
library(earth)
library(reshape2)
library(ggplot2) 
library(gridExtra)
library(quantreg)

setwd('C:/Users/zonde/OneDrive - Nexus365/ROC-CO2-Jesse')

#Data from Codilean et al. 2018 (OCTOPUS dataset) 
#https://doi.org/10.5194/essd-10-2123-2018
#The csv loaded in here contains data from both the International 
#and Australia datasets (which in OCTOPUS are reported seperately)

CRN_dat <- read.csv("CRN_IntAus_ba.csv", header=TRUE)

CRN_dat$EBE_MMKYR[CRN_dat$EBE_MMKYR == -9999.99] <- NA
CRN_dat$SLP_AVE[CRN_dat$SLP_AVE < 0.0] <- NA
CRN_dat$AREA[CRN_dat$AREA < 0] <- NA

#Now we load a table which contains the area coverage of each lithology
#present in the CRN basins. This table was generated using the 
#'Summarize Within' tool in the Geoanalytics Desktop Tools of ArcGIS Pro.
#The CRN basins shapefile provided by Codilean et al. and the
#GLiM shapefile provided by Hartmann & Moosdorf were inputs.
#Hartmann & Moosdorf: https://doi.org/10.1029/2012GC004370
#Note that lithology classes were added to the GLiM shapefile according
#to table S5.

Basin_lith <- read.csv("Basin_lith.csv", header=TRUE)
Basin_lith$percentage <- NA

for(i in 1:3296) #there are 3296 basins in the CRN dataset
{
  x <- Basin_lith$sum_area_squarekilometers[which(Basin_lith$JOIN_ID == i)]
  y <- x/sum(x)
  Basin_lith$percentage[which(Basin_lith$JOIN_ID == i)] <- y
}

#Now let's select those basins with near homogeneous bedrock lithologies
Basin_lith_uniform <- subset(Basin_lith, Basin_lith$percentage >0.80)
nrow(Basin_lith_uniform)
table(Basin_lith_uniform$Unit_id)

#Now we merge this table with the CRN dataset using JOIN_ID.
CRN_lith <- merge(Basin_lith_uniform, CRN_dat, by = 'JOIN_ID')

#let's add Unit_gr to group some of the units for erodibility purposes.
CRN_lith$Unit_gr <- CRN_lith$Unit_id

CRN_lith$Unit_gr[which(CRN_lith$Unit_id == 'c' | CRN_lith$Unit_id == 'sc' | CRN_lith$Unit_id == 'ma' |
                         CRN_lith$Unit_id == 's' | CRN_lith$Unit_id == 'sm' | CRN_lith$Unit_id == 'sh')] <- 'sed'

#And we subset the dataset by rock type:
CRN_ig <- CRN_lith[which(CRN_lith$Unit_gr == 'ig'),]
CRN_m <- CRN_lith[which(CRN_lith$Unit_gr == 'm'),]
CRN_sed <- CRN_lith[which(CRN_lith$Unit_gr == 'sed'),]

#We also load the rasters: 
#These are 1-km grids with values of lithology and median denudation
#Lith is a rasterized version of the GLiM dataset, with numbers 
#corresponding to the lithology classes that we defined (See Table S5)
#1 equals to igneous rocks, 2 equals crystalline metamorphic rocks, and all other numbers
#equal to sedimentary rock types.
#Denud was generated by using the median regression line through basins of each rock type
#(as see below in lines 99-105), the function of which was applied to the 
#Geomorpho90m slope dataset (Amatulli et al. 2020) using the ArcGIS Pro raster calculator.
#Geomorpho90m dataset: https://doi.org/10.1038/s41597-020-0479-6
#Make sure both raster are 1 km resolution, Mollweide WGS84 (equal area) projection,
#with the lithology grid snapped to the denudation grid.
lith = raster("~/OneDrive - University College London/ROC-CO2-Jesse/Monte Carlo/glob_lit_lor2.tif")
denud = raster("~/OneDrive - University College London/ROC-CO2-Jesse/Monte Carlo/glob_den_lor3.tif")

#Create a raster stack
Eros = stack(lith, denud)

#find the unique combination of values in the raster stacks
#We start with the erosion rates
unique_Eros <- unique(Eros)

#Denudation quantiles:

#We choose a 100 values at regular interval between 0 and 1 (0th percentile and
#100th) to build up an empirical quantile function.
taus <- seq(from = 0, to = 1.0, by = 0.01)

#The median regression line used to convert the resampled median denudation raster
#values back to slope (see Supplementary Materials section 2.2). 
#We'll then use this slope value to generate a list of 100 quantile values which
#then form an empirical quantile function for the respective topographic slope
ig_med <- rq(log(EBE_MMKYR) ~ SLP_AVE, data = CRN_ig, tau = 0.5)
m_med <- rq(log(EBE_MMKYR) ~ SLP_AVE, data = CRN_m, tau = 0.5)
sed_med <- rq(log(EBE_MMKYR) ~ SLP_AVE, data = CRN_sed, tau = 0.5)

#Function taking the median denudation and lithology values and generates
#the quantile value specified (e.g. median, 5th percentile, 95th etc.)
Quant2 <- function(tau, denud, lith) {
  medianline <- if(lith == 1){
    ig_med
  } else if (lith == 2){
    m_med
  } else {
    sed_med
  }
  slope <- (log(denud)-medianline$coefficients[[1]])/medianline$coefficients[[2]]
  CRN_data <- if(lith == 1) {
    CRN_ig
  } else if (lith == 2) {
    CRN_m
  } else {
    CRN_sed
  }
  rq <- rq(log(EBE_MMKYR) ~ SLP_AVE, data = CRN_data, tau = tau)
  x <- exp(rq$coefficients[[1]]+(slope * rq$coefficients[[2]]))
  return(x)
}

#Function running Quant2 for all the quantiles specified in the list above ('taus') 
Runall <- function(denuds, liths){
  name <- paste0(as.integer(denuds),'_', as.integer(liths))
  quantiles <- sapply(FUN = Quant2, taus, denuds, liths)
  write.csv(quantiles, paste0('~/OneDrive - University College London/ROC-CO2-Jesse/Monte Carlo/quantiles_glob/', name, '.csv'))
}

#Maps the Runall function across a list of all denudation and lithology value combinations
mapply(FUN = Runall, unique_Eros[,2], unique_Eros[,1])

#Now we have csv files containing a quantile function for each combination of lithology
#and median denudation rate.

#These files will be used for a subroutine Monte Carlo simulation in a python script

